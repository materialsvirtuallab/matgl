{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrates how to refit a MEGNet formation energy model using PyTorch Lightning with MatGL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import zipfile\n",
    "\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from dgl.data.utils import split_dataset\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from pymatgen.core import Structure\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matgl.ext.pymatgen import Structure2Graph, get_element_list\n",
    "from matgl.graph.data import MGLDataLoader, MGLDataset, collate_fn_graph\n",
    "from matgl.layers import BondExpansion\n",
    "from matgl.models import MEGNet\n",
    "from matgl.utils.training import ModelLightningModule\n",
    "\n",
    "# To suppress warnings for clearer output\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "We will download the original dataset used in the training of the MEGNet formation energy model (MP.2018.6.1) from figshare. To make it easier, we will also cache the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to download the dataset\n",
    "def download_file(url: str, filename: str):\n",
    "    \"\"\"Downloads a file from a URL and saves it locally.\"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()  # Raise an error if the request fails\n",
    "\n",
    "    with open(filename, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "\n",
    "# define a function to load the dataset\n",
    "def load_dataset() -> tuple[list[Structure], list[str], list[float]]:\n",
    "    \"\"\"Raw data loading function.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[Structure], list[str], list[float]]: structures, mp_id, Eform_per_atom\n",
    "    \"\"\"\n",
    "    json_filename = \"mp.2018.6.1.json\"\n",
    "    zip_filename = \"mp.2018.6.1.json.zip\"\n",
    "\n",
    "    url = \"https://figshare.com/ndownloader/files/15087992\"\n",
    "\n",
    "    # Download and extract the dataset if it does not exist\n",
    "    if not os.path.exists(json_filename):\n",
    "        print(f\"Downloading dataset from {url}...\")\n",
    "        download_file(url, zip_filename)\n",
    "        with zipfile.ZipFile(zip_filename, \"r\") as zf:\n",
    "            zf.extractall(\".\")\n",
    "        os.remove(zip_filename)  # Clean up the zip file\n",
    "\n",
    "        # Load the data\n",
    "    data = pd.read_json(json_filename)\n",
    "    structures = []\n",
    "    mp_ids = []\n",
    "\n",
    "    for mid, structure_str in tqdm(zip(data[\"material_id\"], data[\"structure\"], strict=False)):\n",
    "        struct = Structure.from_str(structure_str, fmt=\"cif\")\n",
    "        structures.append(struct)\n",
    "        mp_ids.append(mid)\n",
    "\n",
    "    return structures, mp_ids, data[\"formation_energy_per_atom\"].tolist()\n",
    "\n",
    "\n",
    "structures, mp_ids, eform_per_atom = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "For demonstration purposes, we are only going to select 100 structures from the entire set of structures to shorten the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "structures = structures[:100]\n",
    "eform_per_atom = eform_per_atom[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Here, we set up the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get element types in the dataset\n",
    "elem_list = get_element_list(structures)\n",
    "# setup a graph converter\n",
    "converter = Structure2Graph(element_types=elem_list, cutoff=4.0)\n",
    "# convert the raw dataset into MEGNetDataset\n",
    "mp_dataset = MGLDataset(\n",
    "    structures=structures,\n",
    "    labels={\"Eform\": eform_per_atom},\n",
    "    converter=converter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "We will then split the dataset into training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = split_dataset(\n",
    "    mp_dataset,\n",
    "    frac_list=[0.8, 0.1, 0.1],\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")\n",
    "train_loader, val_loader, test_loader = MGLDataLoader(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    collate_fn=collate_fn_graph,\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Model setup\n",
    "\n",
    "In the next step, we setup the model and the ModelLightningModule. Here, we have initialized a MEGNet model from scratch. Alternatively, you can also load one of the pre-trained models for transfer learning, which may speed up the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the embedding layer for node attributes\n",
    "node_embed = torch.nn.Embedding(len(elem_list), 16)\n",
    "# define the bond expansion\n",
    "bond_expansion = BondExpansion(rbf_type=\"Gaussian\", initial=0.0, final=5.0, num_centers=100, width=0.5)\n",
    "\n",
    "# setup the architecture of MEGNet model\n",
    "model = MEGNet(\n",
    "    dim_node_embedding=16,\n",
    "    dim_edge_embedding=100,\n",
    "    dim_state_embedding=2,\n",
    "    nblocks=3,\n",
    "    hidden_layer_sizes_input=(64, 32),\n",
    "    hidden_layer_sizes_conv=(64, 64, 32),\n",
    "    nlayers_set2set=1,\n",
    "    niters_set2set=2,\n",
    "    hidden_layer_sizes_output=(32, 16),\n",
    "    is_classification=False,\n",
    "    activation_type=\"softplus2\",\n",
    "    bond_expansion=bond_expansion,\n",
    "    cutoff=4.0,\n",
    "    gauss_width=0.5,\n",
    ")\n",
    "\n",
    "# setup the MEGNetTrainer\n",
    "lit_module = ModelLightningModule(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Finally, we will initialize the Pytorch Lightning trainer and run the fitting. Note that the max_epochs is set at 20 to demonstrate the fitting on a laptop. A real fitting should use max_epochs > 100 and be run in parallel on GPU resources. For the formation energy, it should be around 2000. The `accelerator=\"cpu\"` was set just to ensure compatibility with M1 Macs. In a real world use case, please remove the kwarg or set it to cuda for GPU based training. You may also need to use `torch.set_default_device(\"cuda\")` or `with torch.device(\"cuda\")` to ensure all data are loaded onto the GPU for training.\n",
    "\n",
    "We have also initialized the Pytorch Lightning Trainer with a `CSVLogger`, which provides a detailed log of the loss metrics at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(\"logs\", name=\"MEGNet_training\")\n",
    "trainer = L.Trainer(max_epochs=20, accelerator=\"cpu\", logger=logger)\n",
    "trainer.fit(model=lit_module, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Visualizing the convergence\n",
    "\n",
    "Finally, we can plot the convergence plot for the loss metrics. You can see that the MAE is already going down nicely with 20 epochs. Obviously, this is nowhere state of the art performance for the formation energies, but a longer training time should lead to results consistent with what was reported in the original MEGNet work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_csv(\"logs/MEGNet_training/version_0/metrics.csv\")\n",
    "metrics[\"train_MAE\"].dropna().plot()\n",
    "metrics[\"val_MAE\"].dropna().plot()\n",
    "\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code just performs cleanup for this notebook.\n",
    "\n",
    "for fn in (\"dgl_graph.bin\", \"lattice.pt\", \"dgl_line_graph.bin\", \"state_attr.pt\", \"labels.json\"):\n",
    "    try:\n",
    "        os.remove(fn)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "shutil.rmtree(\"logs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
