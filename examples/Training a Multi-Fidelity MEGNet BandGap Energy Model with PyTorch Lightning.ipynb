{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrates how to train a Multi-Fidelity MEGNet Band Gap model from scratch using PyTorch Lightning with MatGL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import zipfile\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dgl.data.utils import split_dataset\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from pymatgen.core import Structure\n",
    "\n",
    "from matgl.config import DEFAULT_ELEMENTS\n",
    "from matgl.ext.pymatgen import Structure2Graph\n",
    "from matgl.graph.data import MGLDataLoader, MGLDataset, collate_fn_graph\n",
    "from matgl.models import MEGNet\n",
    "from matgl.utils.training import ModelLightningModule\n",
    "\n",
    "# To suppress warnings for clearer output\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "We will download the original dataset used in the training of the Multi-Fidelity Band Gap model (MP.2019.4.1) from figshare. To make it easier, we will also cache the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, filename):\n",
    "    print(f\"Downloading {filename} from {url} ...\")\n",
    "    response = requests.get(url, allow_redirects=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded successfully: {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {filename}. Status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "## URLs and filenames\n",
    "files_to_download = {\n",
    "    \"https://ndownloader.figshare.com/files/15108200\": \"pymatgen_structures.zip\",\n",
    "    \"https://figshare.com/ndownloader/articles/13040330/versions/1\": \"bandgap_data.zip\",\n",
    "}\n",
    "\n",
    "## Download all files\n",
    "for url, filename in files_to_download.items():\n",
    "    download_file(url, filename)\n",
    "\n",
    "## List your zip files\n",
    "zip_files = [\"pymatgen_structures.zip\", \"bandgap_data.zip\"]\n",
    "\n",
    "for zip_path in zip_files:\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall()  # Extracts into the current folder\n",
    "\n",
    "ALL_FIDELITIES = [\"pbe\", \"gllb-sc\", \"hse\", \"scan\"]\n",
    "\n",
    "## Load the dataset\n",
    "with open(\"mp.2019.04.01.json\") as f:\n",
    "    structure_data = {i[\"material_id\"]: i[\"structure\"] for i in json.load(f)}\n",
    "print(f\"All structures in mp.2019.04.01.json contain {len(structure_data)} structures\")\n",
    "\n",
    "\n",
    "##  Band gap data\n",
    "with gzip.open(\"band_gap_no_structs.gz\", \"rb\") as f:\n",
    "    bandgap_data = json.loads(f.read())\n",
    "\n",
    "useful_ids = set.union(*[set(bandgap_data[i].keys()) for i in ALL_FIDELITIES])  # mp ids that are used in training\n",
    "print(f\"Only {len(useful_ids)} structures are used\")\n",
    "print(\"Calculating the graphs for all structures... this may take minutes.\")\n",
    "structure_data = {i: structure_data[i] for i in useful_ids}\n",
    "structure_data = {i: Structure.from_str(j, fmt=\"cif\") for i, j in structure_data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "In this section, we generate graphs and labels corresponding to the fidelities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Generate graphs and Combined with Fidelity Labels\n",
    "structures = []\n",
    "material_ids = []\n",
    "graph_attrs = []\n",
    "targets = []\n",
    "for fidelity_id, fidelity in enumerate(ALL_FIDELITIES):\n",
    "    for mp_id in bandgap_data[fidelity]:\n",
    "        structure = deepcopy(structure_data[mp_id])\n",
    "\n",
    "        # The fidelity information is included here by changing the state attributes\n",
    "        # PBE: 0, GLLB-SC: 1, HSE: 2, SCAN: 3\n",
    "        graph_attrs.append(fidelity_id)\n",
    "        structures.append(structure)\n",
    "        targets.append(bandgap_data[fidelity][mp_id])\n",
    "        # the new id is of the form mp-id_fidelity, e.g., mp-1234_pbe\n",
    "        material_ids.append(f\"{mp_id}_{fidelity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Here, we set up the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Graph Converter\n",
    "element_types = DEFAULT_ELEMENTS\n",
    "cry_graph = Structure2Graph(element_types, cutoff=5.0)\n",
    "# Define labels for bandgap values\n",
    "labels = {\"bandgap\": targets}\n",
    "dataset = MGLDataset(structures=structures, graph_labels=graph_attrs, labels=labels, converter=cry_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "We will then split the dataset into training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we set 0.1, 0.1 and 0.8 for demonstration purpose to shorten the training time\n",
    "train_data, val_data, test_data = split_dataset(\n",
    "    dataset,\n",
    "    frac_list=[0.1, 0.1, 0.8],\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")\n",
    "my_collate_fn = partial(collate_fn_graph, include_line_graph=False)\n",
    "# Initialize MGLDataLoder\n",
    "train_loader, val_loader, test_loader = MGLDataLoader(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    collate_fn=my_collate_fn,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Model setup\n",
    "\n",
    "In the next step, we setup the model and the ModelLightningModule. Here, we have initialized a MEGNet model from scratch. Alternatively, you can also load one of the pre-trained models for transfer learning, which may speed up the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the MEGNet model\n",
    "model = MEGNet(\n",
    "    element_types=element_types,\n",
    "    cutoff=5.0,\n",
    "    is_intensive=True,\n",
    "    dim_state_embedding=64,\n",
    "    ntypes_state=4,\n",
    "    readout_type=\"set2set\",\n",
    "    include_states=True,\n",
    ")\n",
    "\n",
    "# setup the MEGNetTrainer\n",
    "lit_module = ModelLightningModule(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Finally, we will initialize the Pytorch Lightning trainer and run the fitting. Note that the max_epochs is set at 20 to demonstrate the fitting on a laptop. A real fitting should use max_epochs > 100 and be run in parallel on GPU resources. For the formation energy, it should be around 2000. The `accelerator=\"cpu\"` was set just to ensure compatibility with M1 Macs. In a real world use case, please remove the kwarg or set it to cuda for GPU based training. You may also need to use `torch.set_default_device(\"cuda\")` or `with torch.device(\"cuda\")` to ensure all data are loaded onto the GPU for training.\n",
    "\n",
    "We have also initialized the Pytorch Lightning Trainer with a `CSVLogger`, which provides a detailed log of the loss metrics at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(\"logs\", name=\"MEGNet_training\")\n",
    "trainer = L.Trainer(max_epochs=20, accelerator=\"cpu\", logger=logger)\n",
    "trainer.fit(model=lit_module, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Visualizing the convergence\n",
    "\n",
    "Finally, we can plot the convergence plot for the loss metrics. You can see that the MAE is already going down nicely with 20 epochs. Obviously, this is nowhere state of the art performance for the band gap, but a longer training time should lead to results consistent with what was reported in the original MEGNet work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_csv(\"logs/MEGNet_training/version_0/metrics.csv\")\n",
    "metrics[\"train_MAE\"].dropna().plot()\n",
    "metrics[\"val_MAE\"].dropna().plot()\n",
    "\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code just performs cleanup for this notebook.\n",
    "\n",
    "for fn in (\"dgl_graph.bin\", \"lattice.pt\", \"dgl_line_graph.bin\", \"state_attr.pt\", \"labels.json\"):\n",
    "    try:\n",
    "        os.remove(fn)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "shutil.rmtree(\"logs\")\n",
    "shutil.rmtree(\"MGLDataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
